# Eagle 3

#### This code is based on the HASS code (https://github.com/HArmonizedSS/HASS) and trains models which are similar to the Eagle 3 architecture. It implements the Train Time Test method of Eagle 3. The original Eagle code can be found here: https://github.com/SafeAILab/EAGLE

## To Run:

The training process is broken up into 2 steps: first you generate data from the large model, and second you train the drafter model. It works for Llama 3.1.8B-Instruct and Qwen 3 8B.

### Data Generation Step

#### 1. Download Datasets

**For ShareGPT:**
```bash
wget https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V4.3_unfiltered_cleaned_split.json .
```

**For Ultrachat:** The dataset will be automatically downloaded when you run the generation script.

#### 2. Generate Training Data

The data generation uses the `ge_data.allocation` module to process datasets in parallel across multiple GPUs. You need to run the script separately for each dataset and split combination.

**Key Parameters:**
- `--dataset`: Either `sharegpt` or `ultrachat`
- `--split`: Either `sft` or `gen` (for ultrachat only)
- `--chat_template`: Either `llama` or `qwen` (must match your model)
- `--samples`: Number of samples to process (use larger numbers for full datasets)
- `--total_gpus`: Total number of GPUs available
- `--outdir`: Output directory for generated data, ultrachat can be automatically downloaded.
- `--data_path`: Path to the downloaded dataset file
- `--model_path`: HuggingFace model path

**For Qwen models**, change `--chat_template llama` to `--chat_template qwen` and use the appropriate Qwen model path.

> ðŸ’¡ **Note**: For LLaMA 3.1 8B with full sample counts, this process may generate ~18TB of data. The training script will search the data directory recursively, so the folder structure doesn't need to be flat.

#### 3. Edit gen_data.sh (Optional)

You can modify `gen_data.sh` with your specific parameters and run it instead of the individual commands:

```bash
./gen_data.sh
```

### Training Step

#### 1. Generate Zipf-restricted vocabulary mapping

After data generation, create the vocabulary mapping files:

```bash
./zipf.sh
```

This will create `d2t.npy` and `t2d.npy` files needed for training.

**Configure zipf.sh parameters:**
- `--samples`: Number of examples to use for frequency analysis (recommend 10,000+)
- `--vocab`: Target model vocabulary size (128256 for Llama)
- `--reduced`: Draft model vocabulary size (32000 for Eagle3)
- `--dataDir`: Directory containing your generated data

#### 2. Run training

Update `train.sh` with the appropriate paths to your:
- **Base model** (e.g., `meta-llama/Llama-3.1-8B-Instruct`)
- **Data directory** (e.g., `dataDirectory/`)
- **Training configuration** for your experiment

Launch training:
```bash
./train.sh
```

### Serving the model with vLLM

#### 1. Convert your saved model
1. Modify config path in `convert.sh`, the default is using `CONFIG_PATH="train/llama3_8_B.json"`.
2. Launch conversion by running:
   ```bash
   ./convert.sh
   ```

#### 2. Serve with vLLM
```bash
VLLM_USE_V1=1 vllm serve $PATH_TO_CONVERTED_MODEL
```
Replace `$PATH_TO_CONVERTED_MODEL` with the actual directory path where your converted model was saved.

### TODO:
1. Throw an error if you attempt to create a model that will not be supported in vLLM - with the wrong configuration of heads etc.
